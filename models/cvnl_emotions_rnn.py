# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uMsbk8M0l38RbLpVXanm8Gwuvijk6tBs
"""

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

import random
import torch
from torch.utils.data import Dataset, DataLoader
from collections import defaultdict, Counter
from datasets import load_dataset, ClassLabel
from datasets import Sequence
from better_profanity import profanity
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence
import torch.nn as nn
import re

"""## Dataset"""

# Mapping the go_emotion labels to our new labels
label_mapping = {
    'admiration': 'joy',
    'amusement': 'joy',
    'anger': 'anger',
    'annoyance': 'anger',
    'approval': 'joy',
    'caring': 'love',
    'confusion': 'confusion',
    'curiosity': 'curiosity',
    'desire': 'love',
    'disappointment': 'sadness',
    'disapproval': 'anger',
    'disgust': 'anger',
    'embarrassment': 'sadness',
    'excitement': 'joy',
    'fear': 'fear',
    'gratitude': 'joy',
    'grief': 'sadness',
    'joy': 'joy',
    'love': 'love',
    'nervousness': 'fear',
    'optimism': 'joy',
    'pride': 'joy',
    'realization': 'surprise',
    'relief': 'relief',
    'remorse': 'sadness',
    'sadness': 'sadness',
    'surprise': 'surprise'
}

# Load dataset
dataset = load_dataset("google-research-datasets/go_emotions", "simplified")

# Filter neutral samples
# First, get all original labels from the dataset
original_label_names = dataset['train'].features['labels'].feature.names
# Now, we want to filter out neutral labelled texts
neutral_index = original_label_names.index('neutral')
def filter_neutral(example):
    return neutral_index not in example['labels']

# Now, our dataset splits will not include any texts with the neutral label.
train_split = dataset['train'].filter(filter_neutral)
val_split = dataset['validation'].filter(filter_neutral)
test_split = dataset['test'].filter(filter_neutral)

# Get new number of examples
train_count = len(train_split)
print(f"Original number of test samples: {train_count:,}")
val_count = len(val_split)
print(f"Original number of test samples: {val_count:,}")
test_count = len(test_split)
print(f"Original number of test samples: {test_count:,}")

# Next, get all labels from the training dataset split
filtered_label_names = train_split.info.features['labels'].feature.names
# Remove 'neutral' from filtered_label_names
filtered_label_names = [label for label in filtered_label_names if label != 'neutral']
new_labels_order = ['anger', 'fear', 'joy', 'love', 'sadness', 'surprise',
                    'confusion', 'curiosity', 'relief']
new_label_to_index = {}
for index, name in enumerate(new_labels_order):
  new_label_to_index[name] = index

# Create a list mapping original indices to new indices
original_to_new_index = []
for original_label_name in filtered_label_names:
  new_emotion = label_mapping[original_label_name]
  new_index = new_label_to_index[new_emotion]
  original_to_new_index.append(new_index)

# Function to Map the labels for each example
def map_labels(example):
    # Get the original label numbers
    original_label_index = example['labels']
    # Map each original index to the new index
    new_label_index = []
    for index in original_label_index:
      new_index = original_to_new_index[index]
      new_label_index.append(new_index)
    # Return the new labels in the format the dataset expects
    return {'labels': new_label_index}

mapped_train = train_split.map(map_labels)
mapped_val = val_split.map(map_labels)
mapped_test = test_split.map(map_labels)

# Update dataset features to reflect new labels
new_features = train_split.features.copy()

# Change ClassLabel to Sequence to accommodate list of labels
new_features['labels'] = Sequence(feature=ClassLabel(names=new_labels_order))
mapped_train = mapped_train.cast(new_features)
mapped_val = mapped_val.cast(new_features)
mapped_test = mapped_test.cast(new_features)

print(mapped_train.features['labels'])
print(f"\nTrain samples after partial mapping: {len(mapped_train):,}")

print(mapped_val.features['labels'])
print(f"\nTrain samples after partial mapping: {len(mapped_val):,}")

print(mapped_test.features['labels'])
print(f"\nTrain samples after partial mapping: {len(mapped_test):,}")

# Dictionary to count occurrences of each new label
mapped_train_label_counts = defaultdict(int)
mapped_val_label_counts = defaultdict(int)
mapped_test_label_counts = defaultdict(int)

# Iterate through mapped dataset and count labels
for example in mapped_train:
    for label_idx in example["labels"]:  # Labels are now mapped indices
        label_name = new_labels_order[label_idx]  # Get label name from index
        mapped_train_label_counts[label_name] += 1

for example in mapped_val:
    for label_idx in example["labels"]:  # Labels are now mapped indices
        label_name = new_labels_order[label_idx]  # Get label name from index
        mapped_val_label_counts[label_name] += 1

for example in mapped_test:
    for label_idx in example["labels"]:  # Labels are now mapped indices
        label_name = new_labels_order[label_idx]  # Get label name from index
        mapped_test_label_counts[label_name] += 1


# Print label distribution
print("\nMapped Label Distribution in Train Split:")
for label, count in sorted(mapped_train_label_counts.items(), key=lambda x: -x[1]):  # Sort in descending order
    print(f"{label}: {count:,}")

print("\nMapped Label Distribution in Validation Split:")
for label, count in sorted(mapped_val_label_counts.items(), key=lambda x: -x[1]):  # Sort in descending order
    print(f"{label}: {count:,}")

print("\nMapped Label Distribution in Test Split:")
for label, count in sorted(mapped_test_label_counts.items(), key=lambda x: -x[1]):  # Sort in descending order
    print(f"{label}: {count:,}")

from datasets import concatenate_datasets

def limit_joy_samples(dataset, max_joy, label_idx=2):
    # Split into joy-containing and non-joy samples
    joy_samples = dataset.filter(lambda ex: label_idx in ex['labels'])
    non_joy_samples = dataset.filter(lambda ex: label_idx not in ex['labels'])

    # Calculate actual number to keep
    actual_max = min(max_joy, len(joy_samples))

    # Randomly subsample joy-containing examples
    joy_subset = joy_samples.shuffle(seed=42).select(range(actual_max))

    # Combine with non-joy samples and shuffle
    combined = concatenate_datasets([joy_subset, non_joy_samples]).shuffle(seed=42)
    return combined

# Apply to all splits (joy is index 2 in new_labels_order)
max_joy_train = 6000  # Reduce from ~15k to 6k
max_joy_val = 800     # Reduce from ~2k to 800
max_joy_test = 800     # Reduce from ~1.8k to 800

mapped_train = limit_joy_samples(mapped_train, max_joy_train)
mapped_val = limit_joy_samples(mapped_val, max_joy_val)
mapped_test = limit_joy_samples(mapped_test, max_joy_test)

# Print new distribution
def print_label_distribution(dataset, split_name):
    counts = defaultdict(int)
    for ex in dataset:
        for label in ex['labels']:
            counts[new_labels_order[label]] += 1
    print(f"\n{split_name} Distribution After Balancing:")
    for label, count in sorted(counts.items(), key=lambda x: -x[1]):
        print(f"{label}: {count:,}")

print_label_distribution(mapped_train, "Train")
print_label_distribution(mapped_val, "Validation")
print_label_distribution(mapped_test, "Test")

def better_tokenizer(text):
    # Basic cleaning
    text = text.lower()
    text = text.replace('\n', ' ')       # Remove newlines
    text = text.replace('\t', ' ')       # Remove tabs

    # Simple number replacement
    text = ' '.join(['<num>' if t.isdigit() else t for t in text.split()])

    # Keep only letters and basic punctuation
    cleaned = []
    for char in text:
        if char.isalpha() or char in {"'", " "}:
            cleaned.append(char)
        else:
            cleaned.append(' ')

    return ''.join(cleaned).split()

# Update vocabulary building
word_counter = Counter()
for text in mapped_train['text']:
    tokens = better_tokenizer(text)
    word_counter.update(tokens)

# Build vocab with minimal special tokens
vocab = {"<pad>": 0, "<unk>": 1, "<num>": 2}
for word, _ in word_counter.most_common(20000):  # Limit vocab size
    if word not in vocab:
        vocab[word] = len(vocab)

class EmotionDataset(Dataset):
    def __init__(self, hf_dataset, vocab, num_labels=9):
        """
        hf_dataset: e.g. mapped_train (with 'text' and 'labels')
        vocab: dict mapping word->int
        num_labels: 10 final categories
        """
        self.samples = []
        self.vocab = vocab
        self.num_labels = num_labels

        for ex in hf_dataset:
            text = ex["text"]
            label_idxs = ex["labels"]

            # Tokenize with simple method
            tokens = better_tokenizer(text)

            # Convert to IDs
            token_ids = [vocab.get(tok, 1) for tok in tokens]  # 1 = <unk>

            # Create multi-hot label
            label_vec = [0] * num_labels
            for li in label_idxs:
                if li < num_labels:
                    label_vec[li] = 1

            self.samples.append((
                torch.tensor(token_ids, dtype=torch.long),
                torch.tensor(label_vec, dtype=torch.float)
            ))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx]

def pad_and_pack(batch):
    """
    batch: list of (token_tensor, label_tensor)
    We'll pad the token_tensors to max length in the batch,
    then pack them. We'll keep label_tensors unmodified in a single batch tensor.
    """
    tokens, labels = zip(*batch)

    # Pad tokens
    padded_tokens = pad_sequence(
        tokens,
        batch_first=False,
        padding_value=vocab['<pad>']
    )

    # Pack sequence
    lengths = torch.tensor([len(seq) for seq in tokens])
    packed_tokens = pack_padded_sequence(
        padded_tokens,
        lengths,
        batch_first=False,
        enforce_sorted=False
    )

    # Stack labels
    label_tensor = torch.stack(labels)

    return packed_tokens, label_tensor


class EmbeddingPackable(nn.Module):
    """
    The embedding layer in PyTorch does not support Packed Sequence objects.
    This wrapper class will fix that. If a normal input comes in, it will
    use the regular Embedding layer. Otherwise, it will work on the packed
    sequence to return a new Packed sequence of the appropriate result.
    """
    def __init__(self, embd_layer):
        super(EmbeddingPackable, self).__init__()
        self.embd_layer = embd_layer

    def forward(self, input):
        if type(input) == torch.nn.utils.rnn.PackedSequence:
            # We need to unpack the input,
            sequences, lengths = torch.nn.utils.rnn.pad_packed_sequence(input.cpu(), batch_first=True)
            #Embed it
            sequences = self.embd_layer(sequences.to(input.data.device))
            #And pack it into a new sequence
            return torch.nn.utils.rnn.pack_padded_sequence(sequences, lengths.cpu(),
                                                           batch_first=True, enforce_sorted=False)
        else:#apply to normal data
            return self.embd_layer(input)

# Define EmotionDataset object for each dataset split
train_dataset = EmotionDataset(mapped_train, vocab)
val_dataset   = EmotionDataset(mapped_val,   vocab)
test_dataset  = EmotionDataset(mapped_test,  vocab)

print(f"Train Dataset Split Length: {len(train_dataset)}")
print(f"Validation Dataset Split Length: {len(val_dataset)}")
print(f"Test Dataset Split Length: {len(test_dataset)}")

# DataLoaders with pad_and_pack
batch = 32
train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True, collate_fn=pad_and_pack)
val_loader   = DataLoader(val_dataset,   batch_size=batch, shuffle=False, collate_fn=pad_and_pack)
test_loader  = DataLoader(test_dataset,  batch_size=batch, shuffle=False, collate_fn=pad_and_pack)

# Validate a Sample Batch
sample_batch = next(iter(train_loader))
print(f"\nSample Batch Shapes:")
print("input_ids shape:", sample_batch[0].data.shape)  # Access PackedSequence data and its shape
print("labels shape:", sample_batch[1].shape) # Access label tensor and its shape

# Display first sample in batch (requires unpacking PackedSequence)
print("\nFirst sample in batch:")
# Unpack the PackedSequence to get individual sequences
data, lengths = pad_packed_sequence(sample_batch[0], batch_first=False)
print("Tokens:", data[:, 0]) # print tokens of the first sample
print("Multi-label vector:", sample_batch[1][0]) # Access labels of the first sample

"""## Functions"""

def run_epoch(model, optimizer, data_loader, loss_func, device, results, score_funcs, prefix="", desc=None):
    """
    model -- the PyTorch model / "Module" to run for one epoch
    optimizer -- the object that will update the weights of the network
    data_loader -- DataLoader object that returns tuples of (input, label) pairs.
    loss_func -- the loss function that takes in two arguments, the model outputs and the labels, and returns a score
    device -- the compute lodation to perform training
    score_funcs -- a dictionary of scoring functions to use to evalue the performance of the model
    prefix -- a string to pre-fix to any scores placed into the _results_ dictionary.
    desc -- a description to use for the progress bar.
    """
    running_loss = []
    y_true = []
    y_pred = []
    start = time.time()
    for inputs, labels in tqdm(data_loader, desc=desc, leave=False):
        #Move the batch to the device we are using.
        inputs = moveTo(inputs, device)
        labels = moveTo(labels, device)

        y_hat = model(inputs) #this just computed f_Θ(x(i))
        # Compute loss.
        loss = loss_func(y_hat, labels)

        if model.training:
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

        #Now we are just grabbing some information we would like to have
        running_loss.append(loss.item())

        if len(score_funcs) > 0 and isinstance(labels, torch.Tensor):
            #moving labels & predictions back to CPU for computing / storing predictions
            labels = labels.detach().cpu().numpy()
            y_hat = y_hat.detach().cpu().numpy()
            #add to predictions so far
            y_true.extend(labels.tolist())
            y_pred.extend(y_hat.tolist())
    #end training epoch
    end = time.time()

    ####### CHANGED
    y_pred = np.asarray(y_pred)
    y_true = np.asarray(y_true)

    # Convert model outputs to binary predictions
    y_pred = (y_pred > 0.5).astype(int)  # Convert probabilities to 0 or 1

    # Ensure shapes match
    if y_true.shape != y_pred.shape:
        print(f"Shape Mismatch! y_true: {y_true.shape}, y_pred: {y_pred.shape}")

    # Compute accuracy
    for name, score_func in score_funcs.items():
        try:
            score_value = score_func(y_true, y_pred)
        except Exception as e:
            print(f"Error computing {name}: {e}")
            score_value = float("NaN")

        results[prefix + " " + name].append(score_value)


    #Else, we assume we are working on a regression problem
    #######

    results[prefix + " loss"].append( np.mean(running_loss) )
    return end-start #time spent on epoch

def moveTo(obj, device):
    """
    obj: the python object to move to a device, or to move its contents to a device
    device: the compute device to move objects to
    """
    if hasattr(obj, "to"):
        return obj.to(device)
    elif isinstance(obj, list):
        return [moveTo(x, device) for x in obj]
    elif isinstance(obj, tuple):
        return tuple(moveTo(list(obj), device))
    elif isinstance(obj, set):
        return set(moveTo(list(obj), device))
    elif isinstance(obj, dict):
        to_ret = dict()
        for key, value in obj.items():
            to_ret[moveTo(key, device)] = moveTo(value, device)
        return to_ret
    else:
        return obj

def train_network(model, loss_func, train_loader, val_loader=None, test_loader=None,score_funcs=None,
                         epochs=50, device="cpu", checkpoint_file=None,
                         lr_schedule=None, optimizer=None, disable_tqdm=False
                        ):
    """Train simple neural networks

    Keyword arguments:
    model -- the PyTorch model / "Module" to train
    loss_func -- the loss function that takes in batch in two arguments, the model outputs and the labels, and returns a score
    train_loader -- PyTorch DataLoader object that returns tuples of (input, label) pairs.
    val_loader -- Optional PyTorch DataLoader to evaluate on after every epoch
    test_loader -- Optional PyTorch DataLoader to evaluate on after every epoch
    score_funcs -- A dictionary of scoring functions to use to evalue the performance of the model
    epochs -- the number of training epochs to perform
    device -- the compute lodation to perform training
    lr_schedule -- the learning rate schedule used to alter \eta as the model trains. If this is not None than the user must also provide the optimizer to use.
    optimizer -- the method used to alter the gradients for learning.

    """
    if score_funcs == None:
        score_funcs = {}#Empty set

    to_track = ["epoch", "total time", "train loss"]
    if val_loader is not None:
        to_track.append("val loss")
    if test_loader is not None:
        to_track.append("test loss")
    for eval_score in score_funcs:
        to_track.append("train " + eval_score )
        if val_loader is not None:
            to_track.append("val " + eval_score )
        if test_loader is not None:
            to_track.append("test "+ eval_score )

    total_train_time = 0 #How long have we spent in the training loop?
    results = {}
    #Initialize every item with an empty list
    for item in to_track:
        results[item] = []


    if optimizer == None:
        #The AdamW optimizer is a good default optimizer
        optimizer = torch.optim.AdamW(model.parameters())
        del_opt = True
    else:
        del_opt = False

    #Place the model on the correct compute resource (CPU or GPU)
    model.to(device)
    for epoch in tqdm(range(epochs), desc="Epoch", disable=disable_tqdm):
        model = model.train()#Put our model in training mode

        total_train_time += run_epoch(model, optimizer, train_loader, loss_func, device, results, score_funcs, prefix="train", desc="Training")

        results["epoch"].append( epoch )
        results["total time"].append( total_train_time )


        if val_loader is not None:
            model = model.eval() #Set the model to "evaluation" mode, b/c we don't want to make any updates!
            with torch.no_grad():
                run_epoch(model, optimizer, val_loader, loss_func, device, results, score_funcs, prefix="val", desc="Validating")

        #In PyTorch, the convention is to update the learning rate after every epoch
        if lr_schedule is not None:
            if isinstance(lr_schedule, torch.optim.lr_scheduler.ReduceLROnPlateau):
              if "test loss" in results and len(results["test loss"]) > 0:  # ✅ Check if list exists and has values
                  lr_schedule.step(results["test loss"][-1])
                #lr_schedule.step(results["val loss"][-1])
            else:
                lr_schedule.step()

        if test_loader is not None:
            model = model.eval() #Set the model to "evaluation" mode, b/c we don't want to make any updates!
            with torch.no_grad():
                run_epoch(model, optimizer, test_loader, loss_func, device, results, score_funcs, prefix="test", desc="Testing")


        if checkpoint_file is not None:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'results' : results
                }, checkpoint_file)
    if del_opt:
        del optimizer

    # Find the max length across all tracked metrics
    max_length = max(len(lst) for lst in results.values())

    # Ensure every list is of the same length by appending NaN if missing
    for key in results:
        while len(results[key]) < max_length:
            results[key].append(float("NaN"))  # Fill missing values

    return pd.DataFrame.from_dict(results)

### RNN utility Classes

class LastTimeStep(nn.Module):
    """
    A class for extracting the hidden activations of the last time step following
    the output of a PyTorch RNN module.
    """
    def __init__(self, rnn_layers=1, bidirectional=False):
        super(LastTimeStep, self).__init__()
        self.rnn_layers = rnn_layers
        if bidirectional:
            self.num_driections = 2
        else:
            self.num_driections = 1

    def forward(self, input):
        #Result is either a tupe (out, h_t)
        #or a tuple (out, (h_t, c_t))
        rnn_output = input[0]

        last_step = input[1]
        if(type(last_step) == tuple):
            last_step = last_step[0]
        batch_size = last_step.shape[1] #per docs, shape is: '(num_layers * num_directions, batch, hidden_size)'

        last_step = last_step.view(self.rnn_layers, self.num_driections, batch_size, -1)
        #We want the last layer's results
        last_step = last_step[self.rnn_layers-1]
        #Re order so batch comes first
        last_step = last_step.permute(1, 0, 2)
        #Finally, flatten the last two dimensions into one
        return last_step.reshape(batch_size, -1)

