# -*- coding: utf-8 -*-
"""CVNL Intent (RNN).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Z_uNjETrby9dqxquIOwui5dY8Ha6x1v
"""

import requests
import torch
from torch.utils.data import Dataset
import torch.nn as nn
from collections import defaultdict
from collections import Counter
import kagglehub
import os
import json
from torch.utils.data import DataLoader

def printDataDict(dataDict, newLine):
  if newLine: print()
  print(f"Processed data: {dataDict}")
  print(f"Labels: {dataDict.keys()}")

#SNIPS 2016-12-built-in-intents
snipsBuiltIn = requests.get("https://raw.githubusercontent.com/sonos/nlu-benchmark/refs/heads/master/2016-12-built-in-intents/benchmark_data.json")
snipsBuiltInData = {}
if snipsBuiltIn.status_code == 200:
  data = snipsBuiltIn.json()
  print("Successfully fetched SNIPS built-in-intents data")
  #process the data
  for domain in data["domains"]:
    for intent in domain["intents"]:
      intentName = intent["benchmark"]["Snips"]["original_intent_name"]
      snipsBuiltInData[intentName] = []
      for query in intent["queries"]:
        snipsBuiltInData[intentName].append(query["text"])
  printDataDict(snipsBuiltInData, False)

else:
  print("failed to fetch SNIPS built-in-intents data")

#Chatbots: Intent Recognition Dataset
#https://www.kaggle.com/datasets/elvinagammed/chatbots-intent-recognition-dataset
path = kagglehub.dataset_download("elvinagammed/chatbots-intent-recognition-dataset")
with open(f"{path}/Intent.json", "r") as f:
  data = json.load(f)
chatbotData = {}
for x in data["intents"]:
  intent = x["intent"]
  chatbotData[intent] = []
  for text in x["text"]:
    chatbotData[intent].append(text)

printDataDict(chatbotData, True)

#Clinc150 (small)
#https://github.com/clinc/oos-eval
clinc150Small = requests.get("https://raw.githubusercontent.com/clinc/oos-eval/refs/heads/master/data/data_small.json")
clinc150SmallData = {}
if clinc150Small.status_code == 200:
  data = clinc150Small.json()
  print("Successfully fetched Clinc150 small data")
  #process the data
  for sentence, intent in data["train"]:
    if intent in clinc150SmallData:
      clinc150SmallData[intent].append(sentence)
    else:
      clinc150SmallData[intent] = [sentence]
  printDataDict(clinc150SmallData, True)

else:
  print("failed to fetch SNIPS Clinc150 small data")

'''
dataDict: a dictionary containing the intents and a list of their sentences
key: label (intents)
value: list of sentences
'''
class intentDataset(Dataset):
  def __init__(self, dataDict):
    self.sentences = []
    self.labels = []

    #build dictionaries to convert labels to an id and vice versa
    self.label_to_idx = {label: idx for idx, label in enumerate(dataDict.keys())}
    self.idx_to_label = {idx: label for label, idx in self.label_to_idx.items()}

    #tokenize and prepare data
    tokens = [] #store tokens for building vocab later
    for label, sentences in dataDict.items():
      for sentence in sentences:
          tokenizedSentence = sentence.split() #tokenize
          tokens.extend(tokenizedSentence)
          self.sentences.append(tokenizedSentence)
          self.labels.append(self.label_to_idx[label])

    #build vocab
    self.vocab = {"<pad>": 0, "<unk>": 1}  #reserve special tokens
    for i, word in enumerate(Counter(tokens).keys(), start=2):
        self.vocab[word] = i

    #encode the labels and sentences
    self.encodedSentences = [torch.tensor([self.vocab[word] for word in sentence]) for sentence in self.sentences] #use the vocab to convert the sentences to integers
    self.encodedLabels = torch.tensor(self.labels)

  def __len__(self):
    return len(self.sentences)

  def __getitem__(self, idx):
    return self.encodedSentences[idx], self.encodedLabels[idx]

'''
custom function to combine dataDicts
sometimes, they have the same key
'''
def combineDataDicts(*dataDicts):
  out = {}
  for dataDict in dataDicts:
    for key, value in dataDict.items():
      if key in out:
        out[key].extend(value)
      else:
        out[key] = value
  return out

finalData = combineDataDicts(snipsBuiltInData, chatbotData, clinc150SmallData)
'''
Dataloader time
'''

def pad_and_pack(batch):
  input_tensors = []
  labels = []
  lengths = []
  for x, y in batch:
    input_tensors.append(x)
    labels.append(y)
    lengths.append(x.shape[0]) #Assume shape is (T, *)

  x_padded = torch.nn.utils.rnn.pad_sequence(input_tensors, batch_first=False)
  x_packed = torch.nn.utils.rnn.pack_padded_sequence(x_padded, lengths, batch_first=False, enforce_sorted=False)
  y_batched = torch.as_tensor(labels, dtype=torch.long)
  return x_packed, y_batched

class EmbeddingPackable(nn.Module):
  """
  The embedding layer in PyTorch does not support Packed Sequence objects.
  This wrapper class will fix that. If a normal input comes in, it will
  use the regular Embedding layer. Otherwise, it will work on the packed
  sequence to return a new Packed sequence of the appropriate result.
  """
  def __init__(self, embd_layer):
    super(EmbeddingPackable, self).__init__()
    self.embd_layer = embd_layer

  def forward(self, input):
    if type(input) == torch.nn.utils.rnn.PackedSequence:
      # We need to unpack the input,
      sequences, lengths = torch.nn.utils.rnn.pad_packed_sequence(input.cpu(), batch_first=True)
      #Embed it
      sequences = self.embd_layer(sequences.to(input.data.device))
      #And pack it into a new sequence
      return torch.nn.utils.rnn.pack_padded_sequence(sequences, lengths.cpu(),
                                                      batch_first=True, enforce_sorted=False)
    else:#apply to normal data
      return self.embd_layer(input)

dataset = intentDataset(finalData)
print(f"Dataset Length: {len(dataset)}")

#80% training, 20% test
trainingDataLength = int(len(dataset)*0.8)
trainingData, testingData = torch.utils.data.random_split(dataset, (trainingDataLength, len(dataset)-trainingDataLength))

B = 8 #batch size
train_loader = DataLoader(trainingData, batch_size=B, shuffle=True, collate_fn=pad_and_pack)
test_loader = DataLoader(testingData, batch_size=B, shuffle=False, collate_fn=pad_and_pack)

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from tqdm import tqdm

# For evaluation metrics
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix

# If a GPU is available, we will use it.
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

class IntentClassifierRNN(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim,
                 num_layers=1, bidirectional=False):
        super(IntentClassifierRNN, self).__init__()

        # 1) EmbeddingPackable to handle PackedSequence
        self.embedding = EmbeddingPackable(nn.Embedding(vocab_size, embed_dim))

        # 2) LSTM/GRU RNN layer
        self.rnn = nn.LSTM(input_size=embed_dim,
                           hidden_size=hidden_dim,
                           num_layers=num_layers,
                           batch_first=True,           # We'll pack with batch_first=True
                           bidirectional=bidirectional)

        # 3) Fully-connected output layer
        #    If bidirectional=True, hidden_dim is doubled in the final cat
        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), output_dim)

        self.bidirectional = bidirectional

    def forward(self, x_packed):
        """
        x_packed: A PackedSequence containing the tokenized & padded input.
        """
        # Pass input through embedding
        embedded = self.embedding(x_packed)

        # Pass the embedded sequence into the RNN
        rnn_out, (hidden, cell) = self.rnn(embedded)

        # hidden shape: (num_layers * num_directions, batch_size, hidden_dim)
        # We want the last layer’s hidden state for classification:
        if self.bidirectional:
            # For bidirectional, the last layer’s hidden has shape [2*num_layers, batch_size, hidden_dim]
            # We can concatenate the final forward state and backward state
            forward_hidden = hidden[-2, :, :]  # last layer forward hidden
            backward_hidden = hidden[-1, :, :] # last layer backward hidden
            hidden_cat = torch.cat((forward_hidden, backward_hidden), dim=1)
            logits = self.fc(hidden_cat)
        else:
            # For unidirectional, hidden[-1,:,:] is the last layer’s hidden state
            logits = self.fc(hidden[-1, :, :])

        return logits

# Hyperparameters
vocab_size = len(dataset.vocab)     # size of vocabulary from the dataset
embed_dim  = 100                    # embedding dimension
hidden_dim = 128                    # hidden dimension for RNN
output_dim = len(dataset.label_to_idx)  # number of unique intents
num_layers = 1                      # how many LSTM layers
bidirectional = False               # True or False based on your preference

model = IntentClassifierRNN(
    vocab_size=vocab_size,
    embed_dim=embed_dim,
    hidden_dim=hidden_dim,
    output_dim=output_dim,
    num_layers=num_layers,
    bidirectional=bidirectional
).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

def train_model(model, train_loader, criterion, optimizer, device, epochs=5):
    model.train()  # set model to training mode

    for epoch in range(epochs):
        epoch_loss = 0.0
        for batch_data, batch_labels in tqdm(train_loader,
                                             desc=f"Epoch {epoch+1}/{epochs}",
                                             leave=False):
            # Move data to device
            batch_data = batch_data.to(device)
            batch_labels = batch_labels.to(device)

            # Forward pass
            optimizer.zero_grad()
            logits = model(batch_data)

            # Compute loss
            loss = criterion(logits, batch_labels)

            # Backprop
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_loss = epoch_loss / len(train_loader)
        print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

# Train for a certain number of epochs
train_model(model, train_loader, criterion, optimizer, device, epochs=5)

def evaluate_model(model, test_loader, device):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch_data, batch_labels in test_loader:
            batch_data = batch_data.to(device)
            batch_labels = batch_labels.to(device)

            logits = model(batch_data)  # shape: [batch_size, num_classes]
            predictions = torch.argmax(logits, dim=1)

            all_preds.extend(predictions.cpu().numpy().tolist())
            all_labels.extend(batch_labels.cpu().numpy().tolist())

    # Convert to numpy arrays
    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)

    # Accuracy
    acc = accuracy_score(all_labels, all_preds)

    # Precision, Recall, F1
    # average='weighted' for multi-class problems with class imbalance
    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')

    # Confusion Matrix
    cm = confusion_matrix(all_labels, all_preds)

    print("Evaluation Results:")
    print(f"Accuracy:  {acc:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"F1 Score:  {f1:.4f}")
    print("Confusion Matrix:\n", cm)

evaluate_model(model, test_loader, device)

# Add this at the end of your file
if __name__ == "__main__":
    # Save the trained model
    torch.save({
        'model_state_dict': model.state_dict(),
        'vocab': dataset.vocab,
        'label_to_idx': dataset.label_to_idx,
        'idx_to_label': dataset.idx_to_label,
        'config': {
            'vocab_size': vocab_size,
            'embed_dim': embed_dim,
            'hidden_dim': hidden_dim,
            'output_dim': output_dim,
            'num_layers': num_layers,
            'bidirectional': bidirectional
        }
    }, 'intent_model.pth')